# -*- coding: utf-8 -*-
"""Moringa_Mamba_Group_Project_Week_5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YSxg_yaVO3tw9pAqHG9aarRkXSd-9DJf

## Defining the Question

### a) Specifying the Question

The research question is to identify various factors that lead to staff attrition. 

Additionally, we would like to come up with recommendations to improve staff retention.

### b) Defining the Metric for Success

For this analysis to be considered successful, the following areas must be covered:
1. Overall Exploratory Data Analysis. 
2. Univariate Analysis. 
3. Bivariate Analysis.
4. Multivariate Analysis.
5. Use Appropriate Visualizations.
6.

### c) Understanding the context

Staff attrition refers to the loss of employees through a natural process, such as retirement, resignation, elimination of a position, personal health, or other similar reasons. 

A major problem in high employee attrition is its cost to an organization. Job postings, hiring processes, paperwork and new hire training are some of the common expenses of losing employees and replacing them. 

Another problem posed by attrition is that it may lead to loss of customers as customers often prefer to interact with familiar people. 

Additionally, Errors and issues are more likely if you constantly have new workers.

Given the various drawbacks of attrition on a company, it is important to understand factors that contribute to employees leaving a company so that the company can put measures in place to mitigate this.

### d) Recording the Experimental Design

The following steps will be followed in conducting this study:
1. Defining the Question 
2. Reading the Data.
3. Checking the Data.
4. Performing Univariate and Bivariate Analysis.
5. Creating Visualizations.
6. Implementing The Solution.
7. Conclusions and Recommendations.

### e) Data Relevance

The data provided aligns with the context of our study. This data was extracted from dataworld https://data.world/juhipathak7/hr-attrition-data 

The dataset contains crucial information about the employees working and who worked at the company including their job role, department, level of education, hourly rate, marital status, distance they have to cover from home to work, job satisfaction level, age and how many years they worked with the company.    

All this information will help us analyze if these factors determine whether an employee is most likely to leave the company.

## Reading the Data

### Importing our Libraries
"""

# installing the necessary libraries not in google colab
! pip install https://github.com/pandas-profiling/pandas-profiling/archive/master.zip

# Commented out IPython magic to ensure Python compatibility.
# Let us first import all the libraries we will need for our analysis
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib
from matplotlib import pyplot as plt
# %matplotlib inline
from pandas_profiling import ProfileReport
from scipy import stats
from scipy.stats import norm
from scipy.stats import t
import math
from scipy.stats import ttest_ind
import statsmodels.api as sm
import plotly.express as px

# let us set the warnings that may appear in our analysis off

import warnings
warnings.filterwarnings('ignore') 

# subsequently let us set the pandas warning for chained assignments off
pd.options.mode.chained_assignment = None  # default='warn'

"""### Loading our Dataset"""

# Loading the Dataset from the source i.e. csv
url = '/content/WAFnUseCHREmployeeAttrition (1).csv'

df = pd.read_csv(url)

"""### Previewing the Dataset """

# previewing the first five entries of the dataset

df.head()

# previewing the last five entries of the dataset

df.tail()

"""## Checking the Data"""

# Determining the no. of records in our dataset
#
print('This dataset has ' + str(df.shape[0]) + ' rows, and ' + str(df.shape[1]) + ' columns')

# Checking whether each column has an appropriate datatype
#
df.dtypes

# checking the dataset information

df.info()

# Let us view the summary statistics of our dataset

df.describe()

# let us see the columns in our dataframe
df.columns

# Checking the entire profile of the dataframe

profile = ProfileReport(df, title="Staff Attrition Dataset", html={'style':{'full_width':True}})
profile.to_notebook_iframe()

# let us save our profile report
profile.to_file(output_file="StaffAttrition_Dataset_Profile_Report.html")

"""## Data Cleaning"""

# From our profile report, we can see that we don't have any duplicated rows
# But let us check 
df.duplicated().sum()

# Let Us Drop columns we will not need for this analysis


df.drop(['EmployeeCount','EmployeeNumber', 'Over18', 'StandardHours', 'MonthlyRate'], axis = 1, inplace = True)

# let us confirm that we have dropped the unnecessary columns

df.head()

# Removing the spaces and setting all column names to lower case

df.columns = df.columns.str.lower().str.replace(" ", "_")
df.head()

# From our profile report we can see that there are no null values 
# Let us just check again
df.isnull().sum()

# Checking for Anomalies
# 
# Checking for outliers in the columns with numerical data
col_names = ['age','dailyrate', 'hourlyrate']

fig, ax = plt.subplots(len(col_names), figsize=(5,16))

for i, col_val in enumerate(col_names):
    sns.boxplot(df[col_val], ax=ax[i])
    ax[i].set_title('Box plot - {}'.format(col_val), fontsize=10)
    ax[i].set_xlabel(col_val, fontsize=8)
plt.show()

"""## EXPLORATORY DATA ANALYSIS

### UNIVARIATE ANALYSIS

## a) Numerical Analysis
"""

# finding the information about the variables
df.info()

# finding the outliers of the variables using boxplot

col_names = ['age', 'dailyrate', 'distancefromhome', 'education', 'environmentsatisfaction', 'hourlyrate', 'jobsatisfaction',
             'numcompaniesworked']

fig, ax = plt.subplots(len(col_names), figsize= (8,40))

for i, col_val in enumerate(col_names):
  sns.boxplot(y = df[col_val], ax= ax[i])
  ax[i].set_title('Box plot - {}'.format(col_val), fontsize= 10)
  ax[i].set_xlabel(col_val, fontsize= 8)
plt.show()

# we have outliers in number of companies worked, perfomance rating, total working years and training time last year.

col_names = ['performancerating', 'relationshipsatisfaction', 'totalworkingyears', 'trainingtimeslastyear', 'worklifebalance',
             'yearsatcompany', 'yearsincurrentrole', 'yearssincelastpromotion', 'yearswithcurrmanager']

fig, ax = plt.subplots(len(col_names), figsize= (6,50))

for i, col_val in enumerate(col_names):
  sns.boxplot(y = df[col_val], ax= ax[i])
  ax[i].set_title('Box plot - {}'.format(col_val), fontsize= 10)
  ax[i].set_xlabel(col_val, fontsize= 8)
plt.show()

# Outliers also in performance rating, total working years and training times last year, yearsatcompany, yearsincurrentrole,
# yearssincelastpromotion and yearswithcurrmanager

# checking for anormalities
Q1 = df.quantile(0.25)
Q3 = df.quantile(0.75)

IQR = Q3 - Q1
IQR
lower_bound = Q1 - (1.5*IQR)
upper_bound = Q3 + (1.5*IQR)

print('Lower_Bound' +str(lower_bound))
print('Upper_Bound' +str(upper_bound))

# Removing the outliers

df1 = df[~((df < (Q1 - 1.5 * IQR)) |(df > (Q3 + 1.5 * IQR))).any(axis=1)]
df1.head()

# Getting the size of our data set after removing the outliers
df1.shape

# Afer removing the outliers we remain with 863 rows.

# Removing the outliers will remove alot of data, therefore, we will keep the outliers.
# Furthermore, these outliers seem to be more of a distribution pattern rather than from errors

df.shape

"""### b) Categorical Analysis"""

df.info()

# Bar graph showing travel to work place

df.businesstravel.value_counts().plot.bar()
plt.title('Travel Type')
# most of the employees rarely travel to work

# Piechart showing attrition of the employees

df.attrition.value_counts().plot(kind= 'pie')
plt.title('Left/Retained in the Company')

# Although most of the employees stay in the company a certain number of the employees leave the company

# bar chart showing field of study of the employees

df.educationfield.value_counts().plot.bar()
plt.title('Field of Study')

# most of the employees are in the field of life sciences and the least in the human resources

# bar chart showing male and female workers in the compony

df.gender.value_counts().plot(kind= 'barh')
plt.title('Gender')
# over 50% of the workers are male employees

# barh chart showing role of the employees in the company

df.jobrole.value_counts().plot(kind= 'barh')
plt.title('Role in the company')

# most of the employees are in sales exrcutives, research scientists and laboratory technicians

# bar chart of the marital status

df.maritalstatus.value_counts().plot.bar()
plt.title('Marital Status')

# most of the employees are married

# bar chart showing employees working overtime

df.overtime.value_counts().plot.bar()
plt.title('Work Beyond Set Hours')

# most of the employees do not work overtime

# histogram showing the age distribtion

df['age'].hist()

# most of the employees are of the age between 30 and 40.

"""### c) Summary Statistics"""

df.describe(include= 'all')

# calculating the mean of the dataset

df.mean()

# calculating the median of the dataset

df.median()

# Finding the standard deviation 

df.std()

# The standard deviations shows the ranges, in that the values with more range show higher
# values of standard deviation, meaning the data is more spread away from the mean.

# finding the variance 

df.var()

# finding the skewness 

df.skew()

# most the dataset is negatively skewed
# some of the data is left skewed and posive skew reflect the positively skewed data

# finding the kurtosis of the dataset

df.kurt()

# The data has negative kurtosis indicating that the distribution has tails 
# these means that our outliers are less extreme in these columns but data comes from normal detribution
# we have some positve kurtosis meaning the data has extreme outliers

# df.attrition.replace(to_replace=[0, 1], value=['no', 'yes'])

#df['attrition'] = df['attrition'].astype('category')

#from sklearn.preprocessing import LabelEncoder

#labelencoder = LabelEncoder()

#df['attrition'] = labelencoder.fit_transform(df['attrition'])
#,df['attrition']

"""# BIVARIATE ANALYSIS"""

df.info()

Profile_relation = df.drop(['age', 'dailyrate', 'distancefromhome', 'hourlyrate', 'numcompaniesworked',
                            'percentsalaryhike', 'totalworkingyears', 'trainingtimeslastyear', 'yearsatcompany',
                            'yearsincurrentrole', 'yearssincelastpromotion', 'yearswithcurrmanager'], axis=1)
sns.pairplot(Profile_relation)
plt.show()

# Calculating the pearson coefficient correlation
plt.figure(figsize = (20,10))
sns.heatmap(df.corr(),annot=False)
plt.title('A Heatmap of Pearson Correlation in our Dataset', color='red')
plt.show()

"""# BIVARIATE ANALYSIS RECOMMENDATION

From the Bivariate Analysis and the Profile Report, the columns found to have some correlation were:
Attrition,
BusinessTravel,
Department,
Education,
EducationField,
EnvironmentSatisfaction,
Gender,
JobInvolvement,
JobLevel,
JobRole,
JobSatisfaction,
MaritalStatus,
MonthlyIncome,
OverTime,
PerformanceRating,
RelationshipSatisfaction,
StockOptionLevel and
WorkLifeBalance

# MULTIVARIATE ANALYSIS
"""

df.info()

"""## Principal Component Analysis (PCA)"""

df_pca = df.copy(deep=True)

# Data for label encoding
df_pca['businesstravel'] = df_pca['businesstravel'].astype('category')
df_pca['department'] = df_pca['department'].astype('category')
df_pca['educationfield'] = df_pca['educationfield'].astype('category')
df_pca['gender'] = df_pca['gender'].astype('category')
df_pca['jobrole'] = df_pca['jobrole'].astype('category')
df_pca['maritalstatus'] = df_pca['maritalstatus'].astype('category')
df_pca['overtime'] = df_pca['overtime'].astype('category')

# Label encoding the categorical data

from sklearn.preprocessing import LabelEncoder

labelencoder = LabelEncoder()

df_pca['businesstravel'] = labelencoder.fit_transform(df_pca['businesstravel'])
df_pca['department'] = labelencoder.fit_transform(df_pca['department'])
df_pca['educationfield'] = labelencoder.fit_transform(df_pca['educationfield'])
df_pca['gender'] = labelencoder.fit_transform(df_pca['gender'])
df_pca['jobrole'] = labelencoder.fit_transform(df_pca['jobrole'])
df_pca['maritalstatus'] = labelencoder.fit_transform(df_pca['maritalstatus'])
df_pca['overtime'] = labelencoder.fit_transform(df_pca['overtime'])

x = df_pca.drop(['attrition'], axis= 1) # features
y = df_pca['attrition'] #target variable

# Splitting the data into train and test sets

from sklearn.model_selection import  train_test_split

x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.2, random_state=0)

# Performing standard scalar normalization to normalize our feature set.

from sklearn.preprocessing import StandardScaler

sc = StandardScaler()
x_train =sc.fit_transform(x_train)
x_test = sc.transform (x_test)

# Applying PCA
# We did not specify the number of components in the constructor. 
# Hence, the features in Financial_encoding set will be returned for both the training and test sets.

from sklearn.decomposition import PCA

pca = PCA()
X_train = pca.fit_transform(x_train)
X_test = pca.transform(x_test)

# Explained Variance Ratio

explained_variance = pca.explained_variance_ratio_
explained_variance

x.info()

x.columns

# The explained variance ratio is the percentage of variance that is attributed by each of the selected components. 
# Ideally, you would choose the number of components to include in your model by adding the explained variance ratio 
# of each component until you reach a total of around 0.8 or 80% to avoid overfitting.

first_17 = [0.16263726 +0.06638758 + 0.0632505 + 0.05726161+ 0.05424401+ 0.04109107+ 0.04023643+ 
         0.03870811+0.03696792+ 0.03595021+0.03544304+0.03428372+ 0.03349319+ 0.03296051+ 
         0.03252002+0.0310185+ 0.03065031]

first_17
# This suggests that the first 17 indicate the most determining features to attrition i.e. index 0 to 16 of x.info()

"""# MULTIVARIATE ANALYSIS RECOMMENDATION

From the Multivariate Analysis, using principal component analysis with a cut-off point of 80%, as is the standard recommendation, the columns found to have some correlation were:

â€˜age','businesstravel', 'dailyrate', 'department', 'distancefromhome',
       'education', 'educationfield', 'environmentsatisfaction', 'gender',
       'hourlyrate', 'jobinvolvement', 'joblevel', 'jobrole',
       'jobsatisfaction', 'maritalstatus', 'monthlyincome',
       'Numcompaniesworked'

# Exploratory Data Analysis
Considering the features found with correlation in Bivariate and those found important in Multivariate Analysis with the Target Variable being Attrition, the features to analyze are: 
BusinessTravel,
Department,
Education,
EducationField,
EnvironmentSatisfaction,
Gender,
JobInvolvement,
JobLevel,
JobRole,
JobSatisfaction,
MaritalStatus and
MonthlyIncome

We will however drop Education, EducationField, JobInvolvement and JobRole since they are ordinal with no explanation what their values stand for. We will group them in terms of PerformanceRating where possible to give more insight into the data.

### How Does Business Travel affect Attrition considering Employee Performance
"""

# Business Travel

Travel = df.groupby(['performancerating','attrition'], group_keys=True, as_index=False).apply(lambda x:x.businesstravel.value_counts())
Travel

Travel_Attrition_3 = (Travel.iloc[1, 2:] / (Travel.iloc[1, 2:] + Travel.iloc[0,2:]))*100 
                      

Travel_Attrition_4 = (Travel.iloc[3, 2:] / (Travel.iloc[2, 2:] + Travel.iloc[3, 2:]))*100

Travel_Attrition_3 = Travel_Attrition_3.to_frame()
Travel_Attrition_4 = Travel_Attrition_4.to_frame()
Travel_Attrition = pd.concat([Travel_Attrition_3,Travel_Attrition_4], axis=1)
Travel_Attrition.columns = ['3','4']
Travel_Attrition.plot.barh()

"""Employees who travelled frequently experienced higher attrition rates in general (about 25%). This could be caused by experiencing different cultures that widen their view of the world and the workplace, thereby challenging them to change their job to fit the new mold they are molding their habits into. Employees who did not travel and had a lower performance ranking (3) had the least attrition (about 6%) and this could be due to content in the workplace due to the lack of an outside view of work from different areas and cultures.

### How Does Department affect Attrition considering Employee Performance
"""

# Department

Department = df.groupby(['performancerating','attrition'], group_keys=True, as_index=False).apply(lambda x:x.department.value_counts())
Department

Department_Attrition_3 = (Department.iloc[1, 2:] / (Department.iloc[1, 2:] + Department.iloc[0,2:]))*100 
                      

Department_Attrition_4 = (Department.iloc[3, 2:] / (Department.iloc[2, 2:] + Department.iloc[3, 2:]))*100

Department_Attrition_3 = Department_Attrition_3.to_frame()
Department_Attrition_4 = Department_Attrition_4.to_frame()
Department_Attrition = pd.concat([Department_Attrition_3,Department_Attrition_4], axis=1)
Department_Attrition.columns = ['3','4']
Department_Attrition.plot.barh()

"""Employees with performance rank 3 showed high attrition rates in the Human Resources (20%) and Sales (23%) departments. These are less technical departments. Those with rank 4 had the highest attrition in Sales and Research & Development departments (17%). Research and Development is more technical.

### How Does Environment Satisfaction affect Attrition considering Employee Performance
"""

# Environment Satisfaction

environment = df.groupby(['performancerating','attrition'], 
                         group_keys=True, as_index=True).apply(lambda x:x.environmentsatisfaction.value_counts()).reset_index()
environment

enva_4 = environment[environment.level_2 == 4]
enva_4_group_3 = (enva_4.iloc[1,3] / (enva_4.iloc[0,3]+enva_4.iloc[1,3]))*100
enva_4_group_4 = (enva_4.iloc[3,3] / (enva_4.iloc[2,3]+enva_4.iloc[3,3]))*100

enva_3 = environment[environment.level_2 == 3]
enva_3_group_3 = (enva_3.iloc[1,3] / (enva_3.iloc[0,3]+enva_3.iloc[1,3]))*100
enva_3_group_4 = (enva_3.iloc[3,3] / (enva_3.iloc[2,3]+enva_3.iloc[3,3]))*100

enva_2 = environment[environment.level_2 == 2]
enva_2_group_3 = (enva_2.iloc[1,3] / (enva_2.iloc[0,3]+enva_2.iloc[1,3]))*100
enva_2_group_4 = (enva_2.iloc[3,3] / (enva_2.iloc[2,3]+enva_2.iloc[3,3]))*100

enva_1 = environment[environment.level_2 == 1]
enva_1_group_3 = (enva_1.iloc[1,3] / (enva_1.iloc[0,3]+enva_1.iloc[1,3]))*100
enva_1_group_4 = (enva_1.iloc[3,3] / (enva_1.iloc[2,3]+enva_1.iloc[3,3]))*100

enva_dict = { 'percentage': [enva_4_group_3,enva_4_group_4,enva_3_group_3,enva_3_group_4,
                             enva_2_group_3,enva_2_group_4,enva_1_group_3,enva_1_group_4],
}
enva_df = pd.DataFrame(enva_dict, index= ['Perf_3_Env_4','Perf_4_Env_4','Perf_3_Env_3','Perf_4_Env_3','Perf_3_Env_2',
                        'Perf_4_Env_2','Perf_3_Env_1','Perf_4_Env_1'] )
enva_df

# Perf_4_Env_1 stands for Performance Rating is 4 and Environment Satisfaction is 1. 
# Perf_4_Env_2 stands for Performance Rating is 4 and Environment Satisfaction is 2.
# Perf_3_Env_1 stands for Performance Rating is 3 and Environment Satisfaction is 1. 
# Perf_3_Env_2 stands for Performance Rating is 3 and Environment Satisfaction is 2.
# and so on...

enva_df.plot.barh()

"""Both performance rating groups i.e. 3 and 4, had high attrition rates (25%) if their environment satisfaction was low i.e. 1. This could be dissatisfaction with their work environment including the work atmosphere of the workplace, compared to their performance.

### How Does Gender affect Attrition considering Employee Performance
"""

# Gender

Gender = df.groupby(['performancerating','attrition'], group_keys=True, as_index=False).apply(lambda x:x.gender.value_counts())
Gender

Gender_Attrition_3 = (Gender.iloc[1, 2:] / (Gender.iloc[1, 2:] + Gender.iloc[0,2:]))*100 
                      

Gender_Attrition_4 = (Gender.iloc[3, 2:] / (Gender.iloc[2, 2:] + Gender.iloc[3, 2:]))*100

Gender_Attrition_3 = Gender_Attrition_3.to_frame()
Gender_Attrition_4 = Gender_Attrition_4.to_frame()
Gender_Attrition = pd.concat([Gender_Attrition_3,Gender_Attrition_4], axis=1)
Gender_Attrition.columns = ['3','4']
Gender_Attrition.plot.barh()

"""Males generally had more attrition rates, although only 1-2% more than the females.

### How Does Job Satisfaction affect Attrition considering Employee Performance
"""

# Job Satisfaction

job = df.groupby(['performancerating','attrition'], 
                         group_keys=True, as_index=True).apply(lambda x:x.jobsatisfaction.value_counts()).reset_index()
job

job_4 = job[job.level_2 == 4]
job_4_group_3 = (job_4.iloc[1,3] / (job_4.iloc[0,3]+job_4.iloc[1,3]))*100
job_4_group_4 = (job_4.iloc[3,3] / (job_4.iloc[2,3]+job_4.iloc[3,3]))*100

job_3 = job[job.level_2 == 3]
job_3_group_3 = (job_3.iloc[1,3] / (job_3.iloc[0,3]+job_3.iloc[1,3]))*100
job_3_group_4 = (job_3.iloc[3,3] / (job_3.iloc[2,3]+job_3.iloc[3,3]))*100

job_2 = job[job.level_2 == 2]
job_2_group_3 = (job_2.iloc[1,3] / (job_2.iloc[0,3]+job_2.iloc[1,3]))*100
job_2_group_4 = (job_2.iloc[3,3] / (job_2.iloc[2,3]+job_2.iloc[3,3]))*100

job_1 = job[job.level_2 == 1]
job_1_group_3 = (job_1.iloc[1,3] / (job_1.iloc[0,3]+job_1.iloc[1,3]))*100
job_1_group_4 = (job_1.iloc[3,3] / (job_1.iloc[2,3]+job_1.iloc[3,3]))*100

job_dict = { 'percentage': [job_4_group_3,job_4_group_4,job_3_group_3,job_3_group_4,
                             job_2_group_3,job_2_group_4,job_1_group_3,job_1_group_4],
}
job_df = pd.DataFrame(job_dict, index= ['Perf_3_job_4','Perf_4_job_4','Perf_3_job_3','Perf_4_job_3','Perf_3_job_2',
                        'Perf_4_job_2','Perf_3_job_1','Perf_4_job_1'] )
job_df

# Perf_4_job_1 stands for Performance Rating is 4 and Job Satisfaction is 1. 
# Perf_4_job_2 stands for Performance Rating is 4 and Job Satisfaction is 2.
# Perf_3_job_1 stands for Performance Rating is 3 and Job Satisfaction is 1. 
# Perf_3_job_2 stands for Performance Rating is 3 and Job Satisfaction is 2.
# and so on...

job_df.plot.barh()

"""Performance rating group 4 had high attrition rates (25%) if the job satisfaction was low i.e. 1. This could be dissatisfaction with their day to day activities including compared to their performance. Whereas those in group 4 with high job satisfaction i.e. 4 having the least attrition at 7% indicating relative contentment with their day to day activities including compared to their performance.

### How Does Marital Status affect Attrition considering Employee Performance
"""

# Marital Status
Marital = df.groupby(['performancerating','attrition'], group_keys=True, as_index=True).apply(lambda x:x.maritalstatus.value_counts()).reset_index()
Marital

Marital_Married_3 = Marital[(Marital.performancerating == 3) & (Marital.level_2 == 'Married')]
Marital_Married_3_Attrition = (Marital_Married_3.iloc[1,3] / (Marital_Married_3.iloc[1,3]+ Marital_Married_3.iloc[0,3]))*100

Marital_Married_4 = Marital[(Marital.performancerating == 4) & (Marital.level_2 == 'Married')]
Marital_Married_4_Attrition = (Marital_Married_4.iloc[1,3] / (Marital_Married_4.iloc[1,3]+ Marital_Married_4.iloc[0,3]))*100

Marital_Single_3 = Marital[(Marital.performancerating == 3) & (Marital.level_2 == 'Single')]
Marital_Single_3_Attrition = (Marital_Single_3.iloc[1,3] / (Marital_Single_3.iloc[1,3]+ Marital_Single_3.iloc[0,3]))*100
Marital_Single_3_Attrition

Marital_Single_4 = Marital[(Marital.performancerating == 4) & (Marital.level_2 == 'Single')]
Marital_Single_4_Attrition = (Marital_Single_4.iloc[1,3] / (Marital_Single_4.iloc[1,3]+ Marital_Single_4.iloc[0,3]))*100

Marital_Divorced_3 = Marital[(Marital.performancerating == 3) & (Marital.level_2 == 'Divorced')]
Marital_Divorced_3_Attrition = (Marital_Divorced_3.iloc[1,3] / (Marital_Divorced_3.iloc[1,3]+ Marital_Divorced_3.iloc[0,3]))*100

Marital_Divorced_4 = Marital[(Marital.performancerating == 4) & (Marital.level_2 == 'Divorced')]
Marital_Divorced_4_Attrition = (Marital_Divorced_4.iloc[1,3] / (Marital_Divorced_4.iloc[1,3]+ Marital_Divorced_4.iloc[0,3]))*100
Marital_Divorced_4_Attrition

Marital_dict = { 'percentage': [Marital_Married_3_Attrition,Marital_Married_4_Attrition,Marital_Single_3_Attrition,
                                Marital_Single_4_Attrition,Marital_Divorced_3_Attrition,Marital_Divorced_4_Attrition,]
}

Marital_df = pd.DataFrame(Marital_dict, index= ['Married_3','Married_4','Single_3','Single_4','Divorced_3',
                        'Divorced_4'] )
Marital_df

# Divorced_4 is those Divorced and had the Performance rating of 4
# Divorced_3 is those Divorced and had the Performance rating of 3
# and so on...

Marital_df.plot.barh()

"""Single people in general had the highest attrition rate with those with a performance rating of 4 having an attrition rate as high as 35%. This could be due to their confidence in their ability to find other jobs as well as the lack of dependents, therefore more risk averse. Married people with performance rating of 4 had the least attrition rate at 6% and could be due to being less risk averse due to them having dependents such as a family.

### How Does Monthly Income affect Attrition
"""

# Monthly income attrition

Income_Attrition =df.groupby(['monthlyincome','attrition']).apply(lambda x:x['monthlyincome'].count()).reset_index(name='Counts')
Income_Attrition['monthlyincome']=round(Income_Attrition['monthlyincome'],-3)
Income_Attrition=Income_Attrition.groupby(['monthlyincome','attrition']).apply(lambda x:x['monthlyincome'].count()).reset_index(name='Counts')
fig=px.line(Income_Attrition,x='monthlyincome',y='Counts',color='attrition',title='Monthly Income by Number of Employees')
fig.show()

"""The rate of attrition is highest at the lowest income levels less than 4,000 monthly. The attrition rate decreases with increase in income from around 5,000 monthly, with a minor spike at 10,000 monthly. These individuals represent the midpoint of the payscale and could be looking for better opportunities to earn better from other jobs. The attrition rate flat lines from about 11,000 monthly to 20,000 monthly.

### How Does Age affect Attrition
"""

Age_Attrition =df.groupby(['age','attrition']).apply(lambda x:x['monthlyincome'].count()).reset_index(name='Counts')
px.line(Age_Attrition,x='age',y='Counts',color='attrition',title='Age by Number of Employees')

"""The rate of attrition is highest at the lowest income levels less than 4,000 monthly. The attrition rate decreases with increase in income from around 5,000 monthly, with a minor spike at 10,000 monthly. These individuals represent the midpoint of the payscale and could be looking for better opportunities to earn better from other jobs. The attrition rate flat lines from about 11,000 monthly to 20,000 monthly.

## Hypothesis Testing

#### Hypothesis Testing: Is there significant difference in the means of satisfaction level between employees who had left the company and temployees who had retained in the company?

> H0: Null Hypothesis: (H0: SER = SEL) There is no difference in satisfaction level between employees who had retained and those who left.

> Ha: Alternate Hypothesis: (HA: SER != SEL) There is a difference in satisfaction level between employees who retained and those who left.

> Our significant value = 0.05, Our sample size = 100, we shall use t-statistic
"""

# changing the categorical variables attrition to binary 
df['Attrition'] = df['Attrition'].astype('category')

from sklearn.preprocessing import LabelEncoder

labelencoder = LabelEncoder()

df['Attrition'] = labelencoder.fit_transform(df['Attrition'])
df['Attrition']

# Sorting for No(those who retained) 
df1 = df[df['Attrition'] == 0]
df1.head()

s1 = df1['JobSatisfaction'].sample(n=50, random_state=1)

# Sorting for Yes (those left)
df2 = df[df['Attrition'] == 1]
df2.head()

s2 = df2['JobSatisfaction'].sample(n=50, random_state=1)

# a qq plot of sample 1
fig = sm.qqplot(df1['JobSatisfaction'], fit = True, line= '45')
plt.show()

# a qq plot of sample 2
fig = sm.qqplot(df2['JobSatisfaction'], fit = True, line= '45')
plt.show()

print(s1.mean())

print(s2.mean())

# let us perfrom our ttest
tstat, pval = stats.ttest_rel(s1, s2)
print('t statistic is: ', tstat)
print('p value is: ', pval)

# let us use a loop to analyse our results
if pval<0.05:
    print("Reject null hypothesis")
else:
    print("Accept null hypothesis")

"""Rejecting the null hypothesis means that we have enough statistical evidence to state that, There is a statistically significant difference in satisfaction level between employees who retained and those who left.

# Conclusion

We set out to identify determinant factors that lead to staff attrition in a company. 

From our analysis we have found that:

There is a high number of staff attrition among employees with low job satisfaction levels.

The attrition rate is highest between 28-32 year olds. Young people tend to move from company to company.

The rate of attrition is highest among employees with the lowest income levels i.e. less than $4,000 monthly.

Single people in general had a higher attrition rate compared to married or divorced employees.

Employees in the Human Resources and Sales departments had a higher attrition rate compared to other departments. 

Employees who travelled frequently experienced higher attrition rates in general.

Males generally had more attrition rates, although the difference to females was not that large.

From this analysis, we can conclude that low job satisfaction level, low monthly income, marital status, age and long distance to and from work are among the main causes of staff attrition in the company.
"""